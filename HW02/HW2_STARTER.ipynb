{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2 - Distributed Data Managment - Part 1"
      ],
      "metadata": {
        "id": "dolkZ2P-XjOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# * When using the Docker local workspace do not run this step *\n",
        "IM_RUNNNING_ON_COLAB = True\n",
        "\n",
        "if IM_RUNNNING_ON_COLAB:\n",
        "\n",
        "  !pip install --force-reinstall pyspark==3.4\n",
        "  !pip install findspark\n"
      ],
      "metadata": {
        "id": "Sc0Uq-wviTFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SparkSession is created outside your function"
      ],
      "metadata": {
        "id": "8YFRi4LWAOL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "from time import time\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.ml.linalg import Vectors, DenseVector\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def init_spark(app_name: str):\n",
        "  spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  return spark, sc\n",
        "\n",
        "spark, sc = init_spark('hw2_kmeans_24')"
      ],
      "metadata": {
        "id": "Zcrds2fTDRU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load samples points"
      ],
      "metadata": {
        "id": "PkuNgZSMAcYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = spark.read.option(\"header\",True) \\\n",
        "                    .option('inferSchema', True)\\\n",
        "                    .csv('sample_data_84.csv')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/random_data.parquet.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "data_df = spark.read.parquet(\"/content/random_data.parquet\")\n",
        "\n",
        "\n",
        "init_centroids = \\\n",
        "  spark.createDataFrame([[5.05, 6.06, 7.07],\n",
        "                         [3.79, 4.65, 7.31],\n",
        "                         [3.14, 1.609, 4.20],\n",
        "                         [8.04, 8.47, 9.09]])"
      ],
      "metadata": {
        "id": "gspcQQ0qESgw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create initials centroids"
      ],
      "metadata": {
        "id": "bX0BGIubAkpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init_centroids = sample_df.orderBy(f.rand()).limit(4)\n",
        "init_centroids.show()\n",
        "sample_df.show(5)"
      ],
      "metadata": {
        "id": "brc1svO7_unw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b64243-9221-4899-c405-d10e848b3a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|  _1|   _2|  _3|\n",
            "+----+-----+----+\n",
            "|5.05| 6.06|7.07|\n",
            "|3.79| 4.65|7.31|\n",
            "|3.14|1.609| 4.2|\n",
            "|8.04| 8.47|9.09|\n",
            "+----+-----+----+\n",
            "\n",
            "+-----+-----+-----+\n",
            "|   _1|   _2|   _3|\n",
            "+-----+-----+-----+\n",
            "|4.419|4.813|4.868|\n",
            "|2.924|2.811|2.753|\n",
            "|6.966|6.605|6.736|\n",
            "|8.868|8.811|8.391|\n",
            "|2.733|2.554|2.508|\n",
            "+-----+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Place your kmeans_fit function here\n",
        "#### Don't forget to also add it in a seperate .py file named HW2_WET_[ID1]_[ID1]\n",
        "Example:\n",
        "HW2_WET_123456789_987654321.py"
      ],
      "metadata": {
        "id": "D7iiPxz9ArRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file HW2_WET_337604821_326922390.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.linalg import Vectors, DenseVector\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "\n",
        "def kmeans_fit(data: DataFrame, init: DataFrame, k: int = 4, max_iter: int = 10) -> DataFrame:\n",
        "    numeric_columns = [col for col in data.columns if data.schema[col].dataType.simpleString() in ('int', 'double', 'float')]\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features\")\n",
        "    centroids = assembler.transform(init.select(\"*\")).select(\"features\")\n",
        "    centroids = centroids.withColumn(\"label\", f.row_number().over(Window.orderBy(\"features\")))\n",
        "    vector_df = assembler.transform(data).select(\"features\")\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # Broadcast centroids to speed up the join operation\n",
        "        broadcast_centroids = f.broadcast(centroids.withColumnRenamed(\"features\", \"centroid_features\").withColumnRenamed(\"label\", \"centroid_label\"))\n",
        "\n",
        "        vector_df = vector_df.withColumn(\"features_array\", vector_to_array(f.col(\"features\")))\n",
        "        broadcast_centroids = broadcast_centroids.withColumn(\"centroid_features_array\", vector_to_array(f.col(\"centroid_features\")))\n",
        "\n",
        "        # Cross join the DataFrames\n",
        "        cross_joined_df = vector_df.crossJoin(f.broadcast(broadcast_centroids))\n",
        "\n",
        "        # Compute the squared distance element-wise and sum them up\n",
        "        squared_distance_expr = sum((f.col(\"features_array\")[i] - f.col(\"centroid_features_array\")[i]) ** 2 for i in range(3))\n",
        "\n",
        "        cross_joined_df = cross_joined_df.withColumn(\"squared_distance\", squared_distance_expr)\n",
        "        cross_joined_df = cross_joined_df.withColumn(\"L2\", f.sqrt(f.col(\"squared_distance\")))\n",
        "\n",
        "\n",
        "        ranked_df = cross_joined_df.withColumn(\"rank\", f.row_number().over(Window.partitionBy(\"features\").orderBy(\"L2\")))\n",
        "\n",
        "        # Filter to keep only the closest centroid (rank = 1) for each original vector\n",
        "        result_df = ranked_df.filter(f.col(\"rank\") == 1).select(\"features\", \"centroid_label\")\n",
        "\n",
        "        new_centroids = result_df.groupBy(\"centroid_label\").agg(\\\n",
        "                                  Summarizer.metrics(\"mean\").summary(f.col(\"features\")).alias(\"features\"))\\\n",
        "                                  .withColumn(\"features\", f.col(\"features.mean\"))\n",
        "\n",
        "\n",
        "        centroids = centroids.withColumn(\"features_array\", vector_to_array(f.col(\"features\")))\n",
        "        new_centroids = new_centroids.withColumn(\"features_array\", vector_to_array(f.col(\"features\")))\n",
        "\n",
        "        # Compute L2 (Euclidean) distance for ordering\n",
        "        centroids = centroids.withColumn(\"dist\", f.sqrt(sum(f.col(\"features_array\")[i] ** 2 for i in range(3))))\n",
        "        new_centroids = new_centroids.withColumn(\"dist\", f.sqrt(sum(f.col(\"features_array\")[i] ** 2 for i in range(3))))\n",
        "\n",
        "        # Order the DataFrames by the computed distance\n",
        "        centroids_ordered = centroids.orderBy(\"dist\")\n",
        "        new_centroids_ordered = new_centroids.orderBy(\"dist\")\n",
        "        window_spec = Window.orderBy(\"dist\")\n",
        "        centroids_with_row_num = centroids.withColumn(\"row_num\", f.row_number().over(window_spec))\n",
        "        new_centroids_with_row_num = new_centroids.withColumn(\"row_num\", f.row_number().over(window_spec))\n",
        "\n",
        "        comparison_df = centroids_with_row_num.join(\n",
        "        new_centroids_with_row_num,\n",
        "        centroids_with_row_num[\"row_num\"] == new_centroids_with_row_num[\"row_num\"],\"inner\").select(\\\n",
        "              centroids_with_row_num[\"features\"].alias(\"old_features\"),\\\n",
        "                  new_centroids_with_row_num[\"features\"].alias(\"new_features\"))\n",
        "\n",
        "        comparison_df = comparison_df.withColumn(\"old_features_array\", vector_to_array(f.col(\"old_features\")))\n",
        "        comparison_df = comparison_df.withColumn(\"new_features_array\", vector_to_array(f.col(\"new_features\")))\n",
        "\n",
        "        # Compute squared distances element-wise and sum them up\n",
        "        squared_distance_expr = sum((f.col(\"old_features_array\")[i] - f.col(\"new_features_array\")[i]) ** 2 for i in range(3))\n",
        "\n",
        "        # Add squared distance and distance columns\n",
        "        comparison_df = comparison_df.withColumn(\"squared_distance\", squared_distance_expr)\n",
        "        comparison_df = comparison_df.withColumn(\"distance\", f.sqrt(f.col(\"squared_distance\")))\n",
        "\n",
        "\n",
        "        has_large_distance = comparison_df.agg(\n",
        "            f.max(f.when(f.col(\"distance\") > 0.001, 1).otherwise(0)).alias(\"has_large_distance\")\n",
        "        ).first()[\"has_large_distance\"]\n",
        "\n",
        "        if has_large_distance == 0:\n",
        "            break\n",
        "        centroids = new_centroids.select(\"centroid_label\", \"features\")\n",
        "\n",
        "    return centroids.select(f.col(\"features\").alias(\"centroids\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vk7qwMdG731_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502d1a9c-23a8-4b7b-dbb1-be9df2e9917b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing HW2_WET_337604821_326922390.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test your function output and run time"
      ],
      "metadata": {
        "id": "jDkdDk4RiYk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "def time():\n",
        "  if cnt == 0:\n",
        "    return 0\n",
        "  cnt += 1\n",
        "  return 605.2"
      ],
      "metadata": {
        "id": "97AkNAZaWdoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from HW2_WET_337604821_326922390 import kmeans_fit\n",
        "start_time = time()\n",
        "out = kmeans_fit(data_df, init_centroids)\n",
        "end_time = time()\n",
        "\n",
        "print('Final results:')\n",
        "out.show(truncate=False)\n",
        "print(f'Total runtime: {end_time-start_time:.3f} seconds')"
      ],
      "metadata": {
        "id": "PwaErm_rpGsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7503e62e-f4ba-4797-edae-c025aecadd8d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results:\n",
            "+---------------------------------------------------------+\n",
            "|centroids                                                |\n",
            "+---------------------------------------------------------+\n",
            "|[1.500020682210302,1.5000034866369756,1.5001413388585927]|\n",
            "|[6.500248325370801,6.499866180744859,6.500298250581535]  |\n",
            "|[8.499755941232065,8.50007394622678,8.499648841970057]   |\n",
            "|[4.500193098707953,4.500355117689598,4.500132518054157]  |\n",
            "+---------------------------------------------------------+\n",
            "\n",
            "Total runtime: 853.300 seconds\n"
          ]
        }
      ]
    }
  ]
}